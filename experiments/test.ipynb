{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7383340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "# %cd ../\n",
    "!rm -rf Elastic-Multi-Tier-Intelligence\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbeb42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Elastic-Multi-Tier-Intelligence'...\n",
      "remote: Enumerating objects: 186, done.\u001b[K\n",
      "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
      "remote: Total 186 (delta 76), reused 155 (delta 51), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (186/186), 60.84 KiB | 8.69 MiB/s, done.\n",
      "Resolving deltas: 100% (76/76), done.\n",
      "/content/Elastic-Multi-Tier-Intelligence\n",
      "configs      LICENSE\t     README.md\t       scripts\n",
      "experiments  pyproject.toml  requirements.txt  src\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ashaduzzaman2505ai/Elastic-Multi-Tier-Intelligence.git\n",
    "%cd Elastic-Multi-Tier-Intelligence\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4af183e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 72\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfbe6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.57.3)\n",
      "Requirement already satisfied: datasets>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.0.0)\n",
      "Requirement already satisfied: flower>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: bitsandbytes>=0.45.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.49.1)\n",
      "Requirement already satisfied: accelerate>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
      "Requirement already satisfied: peft>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (6.33.4)\n",
      "Requirement already satisfied: wandb>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.23.1)\n",
      "Requirement already satisfied: codecarbon>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: flwr>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (1.25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: celery>=5.0.5 in /usr/local/lib/python3.12/dist-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (5.6.2)\n",
      "Requirement already satisfied: tornado<7.0.0,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (6.5.1)\n",
      "Requirement already satisfied: prometheus-client>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (0.23.1)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->-r requirements.txt (line 5)) (4.9.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.30.0->-r requirements.txt (line 7)) (7.2.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (4.5.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (2.12.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (2.49.0)\n",
      "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (9.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (13.590.44)\n",
      "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (3.14.3)\n",
      "Requirement already satisfied: questionary in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (2.1.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (13.9.4)\n",
      "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from codecarbon>=2.3.0->-r requirements.txt (line 12)) (0.20.1)\n",
      "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (44.0.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (1.76.0)\n",
      "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.70.0 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (1.76.0)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (0.0.2)\n",
      "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (0.12.1)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (3.23.0)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (2.4.0)\n",
      "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from flwr>=1.11.0->-r requirements.txt (line 14)) (1.2.0)\n",
      "Requirement already satisfied: ray==2.51.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]>=1.11.0->-r requirements.txt (line 15)) (2.51.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 15)) (4.26.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 15)) (1.1.2)\n",
      "Requirement already satisfied: billiard<5.0,>=4.2.1 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: kombu>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.6.2)\n",
      "Requirement already satisfied: vine<6.0,>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: click-didyoumean>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: click-repl>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (0.3.0)\n",
      "Requirement already satisfied: click-plugins>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (1.1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr>=1.11.0->-r requirements.txt (line 14)) (2.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (3.13.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.0->-r requirements.txt (line 11)) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.45.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.16.0->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.16.0->-r requirements.txt (line 11)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.16.0->-r requirements.txt (line 11)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 2)) (2026.1.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon>=2.3.0->-r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon>=2.3.0->-r requirements.txt (line 12)) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->codecarbon>=2.3.0->-r requirements.txt (line 12)) (1.5.4)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon>=2.3.0->-r requirements.txt (line 12)) (2025.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (0.27.2)\n",
      "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (1.5.6)\n",
      "Requirement already satisfied: yaspin in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from questionary->codecarbon>=2.3.0->-r requirements.txt (line 12)) (3.0.52)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr>=1.11.0->-r requirements.txt (line 14)) (2.23)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.0->-r requirements.txt (line 11)) (5.0.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (1.0.9)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (0.16.0)\n",
      "Requirement already satisfied: amqp<6.0.0,>=5.1.1 in /usr/local/lib/python3.12/dist-packages (from kombu>=5.6.0->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon>=2.3.0->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon>=2.3.0->-r requirements.txt (line 12)) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 15)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 15)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 15)) (0.30.0)\n",
      "Requirement already satisfied: termcolor<4.0,>=3.2 in /usr/local/lib/python3.12/dist-packages (from yaspin->fief-client[cli]->codecarbon>=2.3.0->-r requirements.txt (line 12)) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m src.models.edge_model\n",
    "!python -m src.models.cloud_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.utils.escalation_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.agents.edge_coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2e78741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cuda\n",
      "output_dir: ${hydra:runtime.output_dir}\n",
      "num_clients: 5\n",
      "federated_rounds: 5\n",
      "batch_size: 4\n",
      "subset_size: 200\n",
      "data:\n",
      "  dataset_name: truthfulqa/truthful_qa\n",
      "  split: validation\n",
      "  task: multiple_choice\n",
      "model:\n",
      "  edge_model_name: microsoft/Phi-3-mini-4k-instruct\n",
      "  cloud_model_name: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  edge_quantization: 4bit\n",
      "  max_new_tokens: 256\n",
      "  temperature: 0.7\n",
      "federated:\n",
      "  learning_rate: 2.0e-05\n",
      "  local_epochs: 1\n",
      "  communication_tracking: true\n",
      "\n",
      "2026-01-13 07:47:57.642235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768290477.663950   11350 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768290477.670190   11350 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768290477.686387   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768290477.686415   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768290477.686420   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768290477.686424   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-13 07:47:57.691255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "[2026-01-13 07:48:03,573][flwr][WARNING] - DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
      "[2026-01-13 07:48:03,574][flwr][INFO] - Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
      "2026-01-13 07:48:07,962\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'memory': 9225393767.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'accelerator_type:T4': 1.0, 'object_store_memory': 3953740185.0, 'CPU': 2.0, 'node:__internal_head__': 1.0}\n",
      "[2026-01-13 07:48:17,197][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 9225393767.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'accelerator_type:T4': 1.0, 'object_store_memory': 3953740185.0, 'CPU': 2.0, 'node:__internal_head__': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "[2026-01-13 07:48:17,197][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "[2026-01-13 07:48:17,197][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
      "[2026-01-13 07:48:17,384][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "[2026-01-13 07:48:17,384][flwr][INFO] - [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "[2026-01-13 07:48:17,385][flwr][INFO] - Requesting initial parameters from one random client\n",
      "\u001b[36m(pid=11558)\u001b[0m 2026-01-13 07:48:18.438835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=11558)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=11558)\u001b[0m E0000 00:00:1768290498.473423   11558 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=11558)\u001b[0m E0000 00:00:1768290498.483694   11558 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460571   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460610   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460615   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460620   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11558)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(pid=11557)\u001b[0m 2026-01-13 07:48:18.388170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=11557)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=11557)\u001b[0m E0000 00:00:1768290498.420837   11557 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=11557)\u001b[0m E0000 00:00:1768290498.435616   11557 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=11558)\u001b[0m W0000 00:00:1768290498.515772   11558 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(pid=11558)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m         \n",
      "\u001b[36m(pid=11557)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-13 07:48:33,718 E 11421 11421] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 07:48:37,912 E 11509 11509] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[36m(ClientAppActor pid=11558)\u001b[0m [2026-01-13 07:48:47,124 E 11558 11655] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2026-01-13 07:48:47,203 E 11350 11556] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [06:06<06:06, 366.16s/it]\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m [2026-01-13 07:48:47,182 E 11557 11685] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [09:18<00:00, 279.07s/it]\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m WARNING:flwr:Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "[2026-01-13 07:58:12,054][flwr][INFO] - Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:58:12,057][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 0000000045f0b6993dfe4870fa52817942e100d704be7fd97d2bd51798d8ccc6 Worker ID: e6f18293beec79c831f1605b15f7959bbae964121a84657ef64354d8 Node ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28 Worker IP address: 172.28.0.12 Worker port: 46593 Worker PID: 11558 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:16,230][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:16,269][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:17,866][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:17,905][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:20,208][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:20,208][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:24,082][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:24,082][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "[2026-01-13 07:58:24,083][flwr][ERROR] - The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[93mWARNING \u001b[0m:   Actor(e2a16fa8f0d01a819a1febf701000000) will be remove from pool.\n",
      "[2026-01-13 07:58:24,083][flwr][WARNING] - Actor(e2a16fa8f0d01a819a1febf701000000) will be remove from pool.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 290, in _fetch_future_result\n",
      "    raise ex\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "[2026-01-13 07:58:24,084][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 290, in _fetch_future_result\n",
      "    raise ex\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "[2026-01-13 07:58:24,084][flwr][ERROR] - The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 07:58:24,085][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 1 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:58:24,085][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[93mWARNING \u001b[0m:   REMOVED actor e2a16fa8f0d01a819a1febf701000000 from pool\n",
      "[2026-01-13 07:58:24,102][flwr][WARNING] - REMOVED actor e2a16fa8f0d01a819a1febf701000000 from pool\n",
      "\u001b[93mWARNING \u001b[0m:   Pool size: 1\n",
      "[2026-01-13 07:58:24,107][flwr][WARNING] - Pool size: 1\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2956, in get\n",
      "    raise ValueError(\n",
      "ValueError: Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "\n",
      "[2026-01-13 07:58:24,309][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2956, in get\n",
      "    raise ValueError(\n",
      "ValueError: Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "[2026-01-13 07:58:24,349][flwr][ERROR] - Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:26,300][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:26,336][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      The cluster expanded. Adding 1 actors to the pool.\n",
      "[2026-01-13 07:58:27,512][flwr][INFO] - The cluster expanded. Adding 1 actors to the pool.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:28,278][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:28,316][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:30,257][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:30,275][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:32,340][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:32,341][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 2159 MiB, 1 objects, write throughput 135 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(pid=235486)\u001b[0m 2026-01-13 07:58:57.034821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=235486)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=235486)\u001b[0m E0000 00:00:1768291137.794807  235486 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=235486)\u001b[0m E0000 00:00:1768291137.911874  235486 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924044  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924119  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924123  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924127  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:00,926][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:00,926][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,927][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,928][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 07:59:07,923 E 11509 11509] (raylet) node_manager.cc:3252: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m [2026-01-13 07:59:24,004 E 235486 235534] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:50,907][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:50,908][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 07:59:50,913][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 2 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:59:50,913][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:59,391][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:59,391][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:01,589][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:01,590][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:01,590][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,590][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,630][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,591][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,591][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,630][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:01,731][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:00:01,732][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "[2026-01-13 08:00:01,732][flwr][INFO] - [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:01,732][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:03,972][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:04,009][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:05,916][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:05,953][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 08:00:07,925 E 11509 11509] (raylet) node_manager.cc:3252: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:11,622][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:11,622][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:11,624][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:11,624][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:11,625][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:11,663][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:11,664][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 3 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:11,664][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,153][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,300][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,300][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,326][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,326][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,327][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,376][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,440][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:22,100][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:22,100][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:22,101][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:00:22,102][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "[2026-01-13 08:00:22,102][flwr][INFO] - [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:22,102][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:24,322][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:24,331][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:26,444][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:26,444][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:28,797][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:28,837][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:30,782][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:30,821][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:32,533][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:32,533][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:32,534][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 4 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:32,534][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:34,734][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:34,774][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:36,726][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:36,763][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:42,896][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:42,896][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:42,899][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:42,899][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:42,899][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:42,900][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:42,998][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:00:42,998][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "[2026-01-13 08:00:42,998][flwr][INFO] - [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:42,999][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:45,238][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:45,276][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:47,184][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:47,222][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:51,075][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:51,248][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:53,441][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 5 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:53,441][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:55,650][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:55,680][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:57,619][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:57,658][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:01:03,449][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:01:03,449][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:01:03,451][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:01:03,451][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:01:03,488][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:01:03,488][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:01:03,589][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 171.53s\n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - Run finished 5 round(s) in 171.53s\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - \n",
      "Federated simulation complete.\n",
      "Total reasoning summaries collected: 0\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python scripts/train_federated.py subset_size=50 num_clients=4 federated_rounds=2\n",
    "!python -m scripts.train_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee001b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
