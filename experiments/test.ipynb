{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7383340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Elastic-Multi-Tier-Intelligence\n",
      "LICENSE    configs\toutputs\t\trequirements.txt  src\n",
      "README.md  experiments\tpyproject.toml\tscripts\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "# !rm -rf Elastic-Multi-Tier-Intelligence\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbeb42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Elastic-Multi-Tier-Intelligence'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 196, done.\u001b[K\n",
      "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
      "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
      "remote: Total 196 (delta 81), reused 163 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (196/196), 78.55 KiB | 11.22 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "/workspaces/Elastic-Multi-Tier-Intelligence/experiments/Elastic-Multi-Tier-Intelligence\n",
      "LICENSE    configs\tpyproject.toml\t  scripts\n",
      "README.md  experiments\trequirements.txt  src\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ashaduzzaman2505ai/Elastic-Multi-Tier-Intelligence.git\n",
    "%cd Elastic-Multi-Tier-Intelligence\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af183e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 30 (8.7 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfbe6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (4.57.3)\n",
      "Requirement already satisfied: datasets>=2.18.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: flower>=1.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: hydra-core>=1.3.2 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: bitsandbytes>=0.45.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.49.1)\n",
      "Requirement already satisfied: accelerate>=0.30.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
      "Requirement already satisfied: peft>=0.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: sentencepiece in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (6.33.4)\n",
      "Requirement already satisfied: wandb>=0.16.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.23.1)\n",
      "Requirement already satisfied: omegaconf in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: flwr>=1.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (1.25.0)\n",
      "Requirement already satisfied: codecarbon>=2.7.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.2.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->-r requirements.txt (line 1)) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/codespace/.local/lib/python3.12/site-packages (from transformers>=4.45.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.45.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 3)) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.18.0->-r requirements.txt (line 3)) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.18.0->-r requirements.txt (line 3)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.18.0->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.18.0->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: celery>=5.0.5 in /home/codespace/.local/lib/python3.12/site-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (5.6.2)\n",
      "Requirement already satisfied: tornado<7.0.0,>=5.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (6.5.4)\n",
      "Requirement already satisfied: prometheus-client>=0.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (0.24.0)\n",
      "Requirement already satisfied: humanize in /home/codespace/.local/lib/python3.12/site-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: pytz in /home/codespace/.local/lib/python3.12/site-packages (from flower>=1.8.0->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/codespace/.local/lib/python3.12/site-packages (from hydra-core>=1.3.2->-r requirements.txt (line 5)) (4.9.3)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.30.0->-r requirements.txt (line 7)) (7.2.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (4.5.1)\n",
      "Requirement already satisfied: pydantic<3 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.16.0->-r requirements.txt (line 11)) (2.49.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3->wandb>=0.16.0->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3->wandb>=0.16.0->-r requirements.txt (line 11)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3->wandb>=0.16.0->-r requirements.txt (line 11)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (44.0.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (1.76.0)\n",
      "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.70.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (1.76.0)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (0.0.2)\n",
      "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (3.23.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (13.9.4)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (2.4.0)\n",
      "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: typer<0.21.0,>=0.12.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr>=1.11.0->-r requirements.txt (line 13)) (0.20.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/codespace/.local/lib/python3.12/site-packages (from cryptography<45.0.0,>=44.0.1->flwr>=1.11.0->-r requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich<14.0.0,>=13.5.0->flwr>=1.11.0->-r requirements.txt (line 13)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich<14.0.0,>=13.5.0->flwr>=1.11.0->-r requirements.txt (line 13)) (2.19.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from typer<0.21.0,>=0.12.5->flwr>=1.11.0->-r requirements.txt (line 13)) (1.5.4)\n",
      "Requirement already satisfied: arrow in /home/codespace/.local/lib/python3.12/site-packages (from codecarbon>=2.7.0->-r requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: fief-client[cli] in /home/codespace/.local/lib/python3.12/site-packages (from codecarbon>=2.7.0->-r requirements.txt (line 15)) (0.20.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/codespace/.local/lib/python3.12/site-packages (from codecarbon>=2.7.0->-r requirements.txt (line 15)) (9.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/codespace/.local/lib/python3.12/site-packages (from codecarbon>=2.7.0->-r requirements.txt (line 15)) (13.590.44)\n",
      "Requirement already satisfied: rapidfuzz in /home/codespace/.local/lib/python3.12/site-packages (from codecarbon>=2.7.0->-r requirements.txt (line 15)) (3.14.3)\n",
      "Requirement already satisfied: questionary in /home/codespace/.local/lib/python3.12/site-packages (from codecarbon>=2.7.0->-r requirements.txt (line 15)) (2.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: billiard<5.0,>=4.2.1 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: kombu>=5.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.6.2)\n",
      "Requirement already satisfied: vine<6.0,>=5.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: click-didyoumean>=0.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: click-repl>=0.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (0.3.0)\n",
      "Requirement already satisfied: click-plugins>=1.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (1.1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzlocal in /home/codespace/.local/lib/python3.12/site-packages (from celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.12/site-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr>=1.11.0->-r requirements.txt (line 13)) (2.23)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.36 in /home/codespace/.local/lib/python3.12/site-packages (from click-repl>=0.2.0->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (3.0.52)\n",
      "Requirement already satisfied: ray==2.51.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from flwr[simulation]>=1.11.0->-r requirements.txt (line 14)) (2.51.1)\n",
      "Requirement already satisfied: jsonschema in /home/codespace/.local/lib/python3.12/site-packages (from ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 14)) (4.25.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 14)) (1.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.0->-r requirements.txt (line 11)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.0->-r requirements.txt (line 11)) (5.0.2)\n",
      "Requirement already satisfied: amqp<6.0.0,>=5.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from kombu>=5.6.0->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tzdata>=2025.2 in /home/codespace/.local/lib/python3.12/site-packages (from kombu>=5.6.0->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (2025.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr>=1.11.0->-r requirements.txt (line 13)) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->celery>=5.0.5->flower>=1.8.0->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.3.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from fief-client[cli]->codecarbon>=2.7.0->-r requirements.txt (line 15)) (1.5.6)\n",
      "Requirement already satisfied: yaspin in /home/codespace/.local/lib/python3.12/site-packages (from fief-client[cli]->codecarbon>=2.7.0->-r requirements.txt (line 15)) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema->ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 14)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema->ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 14)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema->ray==2.51.1->flwr[simulation]>=1.11.0->-r requirements.txt (line 14)) (0.29.0)\n",
      "Requirement already satisfied: termcolor<4.0,>=3.2 in /home/codespace/.local/lib/python3.12/site-packages (from yaspin->fief-client[cli]->codecarbon>=2.7.0->-r requirements.txt (line 15)) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530e1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "LocalTokenNotFoundError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `hf auth login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLocalTokenNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[39m, in \u001b[36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m         message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + custom_message\n\u001b[32m    100\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:31\u001b[39m, in \u001b[36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m extra_args = \u001b[38;5;28mlen\u001b[39m(args) - \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extra_args <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[32m     33\u001b[39m args_msg = [\n\u001b[32m     34\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[-extra_args:])\n\u001b[32m     36\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/_login.py:120\u001b[39m, in \u001b[36mlogin\u001b[39m\u001b[34m(token, add_to_git_credential, new_session, write_permission)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m add_to_git_credential:\n\u001b[32m    114\u001b[39m         logger.info(\n\u001b[32m    115\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe token has not been saved to the git credentials helper. Pass \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`add_to_git_credential=True` in this function directly or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`--add-to-git-credential` if using via `hf`CLI if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33myou want to set the git credential as well.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[32m    122\u001b[39m     notebook_login(new_session=new_session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/_login.py:398\u001b[39m, in \u001b[36m_login\u001b[39m\u001b[34m(token, add_to_git_credential)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token.startswith(\u001b[33m\"\u001b[39m\u001b[33mapi_org\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou must use your personal account token, not an organization token.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m token_info = \u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m permission = token_info[\u001b[33m\"\u001b[39m\u001b[33mauth\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maccessToken\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    400\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mToken is valid (permission: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpermission\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1797\u001b[39m, in \u001b[36mHfApi.whoami\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1793\u001b[39m \u001b[38;5;66;03m# Get the effective token using the helper function get_token\u001b[39;00m\n\u001b[32m   1794\u001b[39m effective_token = token \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token \u001b[38;5;129;01mor\u001b[39;00m get_token() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1795\u001b[39m r = get_session().get(\n\u001b[32m   1796\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/whoami-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     headers=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_token\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1798\u001b[39m )\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1800\u001b[39m     hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/hf_api.py:9518\u001b[39m, in \u001b[36mHfApi._build_hf_headers\u001b[39m\u001b[34m(self, token, library_name, library_version, user_agent)\u001b[39m\n\u001b[32m   9515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9516\u001b[39m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[32m   9517\u001b[39m     token = \u001b[38;5;28mself\u001b[39m.token\n\u001b[32m-> \u001b[39m\u001b[32m9518\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9522\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[39m, in \u001b[36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m         message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + custom_message\n\u001b[32m    100\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_headers.py:126\u001b[39m, in \u001b[36mbuild_hf_headers\u001b[39m\u001b[34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33;03m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m token_to_send = \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n\u001b[32m    129\u001b[39m hf_headers = {\n\u001b[32m    130\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser-agent\u001b[39m\u001b[33m\"\u001b[39m: _http_user_agent(\n\u001b[32m    131\u001b[39m         library_name=library_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m     )\n\u001b[32m    135\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_headers.py:159\u001b[39m, in \u001b[36mget_token_to_send\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LocalTokenNotFoundError(\n\u001b[32m    160\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `hf auth login` or `huggingface_hub.login`. See\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/settings/tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m         )\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[31mLocalTokenNotFoundError\u001b[39m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `hf auth login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m src.models.edge_model\n",
    "!python -m src.models.cloud_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.utils.escalation_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.agents.edge_coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2e78741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cuda\n",
      "output_dir: ${hydra:runtime.output_dir}\n",
      "num_clients: 5\n",
      "federated_rounds: 5\n",
      "batch_size: 4\n",
      "subset_size: 200\n",
      "data:\n",
      "  dataset_name: truthfulqa/truthful_qa\n",
      "  split: validation\n",
      "  task: multiple_choice\n",
      "model:\n",
      "  edge_model_name: microsoft/Phi-3-mini-4k-instruct\n",
      "  cloud_model_name: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  edge_quantization: 4bit\n",
      "  max_new_tokens: 256\n",
      "  temperature: 0.7\n",
      "federated:\n",
      "  learning_rate: 2.0e-05\n",
      "  local_epochs: 1\n",
      "  communication_tracking: true\n",
      "\n",
      "2026-01-13 07:47:57.642235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768290477.663950   11350 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768290477.670190   11350 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768290477.686387   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768290477.686415   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768290477.686420   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768290477.686424   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-13 07:47:57.691255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "[2026-01-13 07:48:03,573][flwr][WARNING] - DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
      "[2026-01-13 07:48:03,574][flwr][INFO] - Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
      "2026-01-13 07:48:07,962\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'memory': 9225393767.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'accelerator_type:T4': 1.0, 'object_store_memory': 3953740185.0, 'CPU': 2.0, 'node:__internal_head__': 1.0}\n",
      "[2026-01-13 07:48:17,197][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 9225393767.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'accelerator_type:T4': 1.0, 'object_store_memory': 3953740185.0, 'CPU': 2.0, 'node:__internal_head__': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "[2026-01-13 07:48:17,197][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "[2026-01-13 07:48:17,197][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
      "[2026-01-13 07:48:17,384][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "[2026-01-13 07:48:17,384][flwr][INFO] - [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "[2026-01-13 07:48:17,385][flwr][INFO] - Requesting initial parameters from one random client\n",
      "\u001b[36m(pid=11558)\u001b[0m 2026-01-13 07:48:18.438835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=11558)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=11558)\u001b[0m E0000 00:00:1768290498.473423   11558 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=11558)\u001b[0m E0000 00:00:1768290498.483694   11558 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460571   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460610   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460615   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11557)\u001b[0m W0000 00:00:1768290498.460620   11557 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=11558)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(pid=11557)\u001b[0m 2026-01-13 07:48:18.388170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=11557)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=11557)\u001b[0m E0000 00:00:1768290498.420837   11557 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=11557)\u001b[0m E0000 00:00:1768290498.435616   11557 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=11558)\u001b[0m W0000 00:00:1768290498.515772   11558 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(pid=11558)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m         \n",
      "\u001b[36m(pid=11557)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-13 07:48:33,718 E 11421 11421] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 07:48:37,912 E 11509 11509] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[36m(ClientAppActor pid=11558)\u001b[0m [2026-01-13 07:48:47,124 E 11558 11655] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2026-01-13 07:48:47,203 E 11350 11556] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "Loading checkpoint shards:  50%|     | 1/2 [06:06<06:06, 366.16s/it]\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m [2026-01-13 07:48:47,182 E 11557 11685] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "Loading checkpoint shards: 100%|| 2/2 [09:18<00:00, 279.07s/it]\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "\u001b[36m(ClientAppActor pid=11557)\u001b[0m WARNING:flwr:Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "[2026-01-13 07:58:12,054][flwr][INFO] - Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "[2026-01-13 07:58:12,055][flwr][INFO] - [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:58:12,057][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 0000000045f0b6993dfe4870fa52817942e100d704be7fd97d2bd51798d8ccc6 Worker ID: e6f18293beec79c831f1605b15f7959bbae964121a84657ef64354d8 Node ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28 Worker IP address: 172.28.0.12 Worker port: 46593 Worker PID: 11558 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:16,230][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:16,269][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:17,866][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:17,905][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:20,208][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:20,208][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:24,082][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:24,082][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "[2026-01-13 07:58:24,083][flwr][ERROR] - The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[93mWARNING \u001b[0m:   Actor(e2a16fa8f0d01a819a1febf701000000) will be remove from pool.\n",
      "[2026-01-13 07:58:24,083][flwr][WARNING] - Actor(e2a16fa8f0d01a819a1febf701000000) will be remove from pool.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 290, in _fetch_future_result\n",
      "    raise ex\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "[2026-01-13 07:58:24,084][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 290, in _fetch_future_result\n",
      "    raise ex\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "[2026-01-13 07:58:24,084][flwr][ERROR] - The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ClientAppActor\n",
      "\tactor_id: e2a16fa8f0d01a819a1febf701000000\n",
      "\tpid: 11558\n",
      "\tnamespace: c860cf88-c5de-4056-a3ab-5b13ff055667\n",
      "\tip: 172.28.0.12\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 07:58:24,085][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 1 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:58:24,085][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[93mWARNING \u001b[0m:   REMOVED actor e2a16fa8f0d01a819a1febf701000000 from pool\n",
      "[2026-01-13 07:58:24,102][flwr][WARNING] - REMOVED actor e2a16fa8f0d01a819a1febf701000000 from pool\n",
      "\u001b[93mWARNING \u001b[0m:   Pool size: 1\n",
      "[2026-01-13 07:58:24,107][flwr][WARNING] - Pool size: 1\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2956, in get\n",
      "    raise ValueError(\n",
      "ValueError: Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "\n",
      "[2026-01-13 07:58:24,309][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2956, in get\n",
      "    raise ValueError(\n",
      "ValueError: Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "[2026-01-13 07:58:24,349][flwr][ERROR] - Invalid type of object refs, <class 'NoneType'>, is given. 'object_refs' must either be an ObjectRef or a list of ObjectRefs. \n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:26,300][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:26,336][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      The cluster expanded. Adding 1 actors to the pool.\n",
      "[2026-01-13 07:58:27,512][flwr][INFO] - The cluster expanded. Adding 1 actors to the pool.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:28,278][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:28,316][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:30,257][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:30,275][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:58:32,340][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:58:32,341][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:58:32,343][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 2159 MiB, 1 objects, write throughput 135 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(pid=235486)\u001b[0m 2026-01-13 07:58:57.034821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=235486)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=235486)\u001b[0m E0000 00:00:1768291137.794807  235486 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=235486)\u001b[0m E0000 00:00:1768291137.911874  235486 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924044  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924119  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924123  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m W0000 00:00:1768291138.924127  235486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:00,926][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:00,926][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,927][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,928][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:00,929][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 07:59:07,923 E 11509 11509] (raylet) node_manager.cc:3252: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(pid=235486)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m [2026-01-13 07:59:24,004 E 235486 235534] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\u001b[36m(ClientAppActor pid=235486)\u001b[0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:50,907][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:50,908][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 07:59:50,913][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 2 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 07:59:50,913][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 07:59:59,391][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 07:59:59,391][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:01,589][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:01,590][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:01,590][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,590][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,630][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,591][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,591][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:01,630][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:01,731][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:00:01,732][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "[2026-01-13 08:00:01,732][flwr][INFO] - [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:01,732][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:03,972][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:04,009][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:05,916][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:05,953][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-13 08:00:07,925 E 11509 11509] (raylet) node_manager.cc:3252: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:11,622][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:11,622][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:11,624][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:11,624][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:11,625][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:11,663][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:11,664][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 3 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:11,664][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,153][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,300][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,300][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,326][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:20,326][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,327][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,376][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:20,440][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:22,100][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:22,100][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:22,101][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:00:22,102][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "[2026-01-13 08:00:22,102][flwr][INFO] - [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:22,102][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:24,322][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:24,331][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:26,444][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:26,444][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:28,797][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:28,837][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:30,782][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:30,821][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:32,533][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:32,533][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:32,534][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 4 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:32,534][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:34,734][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:34,774][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:36,726][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:36,763][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:42,896][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:42,896][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:42,899][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:42,899][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:42,899][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:42,900][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:42,998][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:00:42,998][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "[2026-01-13 08:00:42,998][flwr][INFO] - [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:42,999][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:45,238][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:45,276][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:47,184][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:47,222][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:51,075][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:51,248][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:53,386][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 5 failures\n",
      "[2026-01-13 08:00:53,441][flwr][INFO] - aggregate_fit: received 0 results and 5 failures\n",
      "Round 5 | Collected 0 reasoning summaries\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "[2026-01-13 08:00:53,441][flwr][INFO] - configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:55,650][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:55,680][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:00:57,619][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:00:57,658][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:01:03,449][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:01:03,449][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:01:03,451][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "[2026-01-13 08:01:03,451][flwr][ERROR] - Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 94, in _submit_job\n",
      "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 400, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 281, in _fetch_future_result\n",
      "    res_cid, out_mssg, updated_context = ray.get(\n",
      "                                         ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:01:03,488][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "[2026-01-13 08:01:03,488][flwr][ERROR] - Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 6e3ab2e56808b2a757665f9debb779d3ccccf518e0f48a2b84b9ce28) where the lease (lease ID: 0200000043a04074242218030caebd9a9eddbdb09c1d71d74ec4e9c3591de3de, name=ClientAppActor.__init__, pid=235486, memory used=5.49GB) was running was 12.09GB / 12.67GB (0.953907), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-df4e3ef0a5e7f9d8fc5563708fded9378916a89a50e52dfa19da63b6*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "235486\t5.49\t\n",
      "11350\t2.81\tpython3 -m scripts.train_federated\n",
      "115\t0.12\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
      "420\t0.12\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a0560fbe-d9dc...\n",
      "11456\t0.08\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
      "11536\t0.08\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
      "11455\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
      "11508\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
      "11538\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
      "235508\t0.06\tray::IDLE_SpillWorker\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 5 failures\n",
      "[2026-01-13 08:01:03,589][flwr][INFO] - aggregate_evaluate: received 0 results and 5 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 171.53s\n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - Run finished 5 round(s) in 171.53s\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "[2026-01-13 08:01:03,590][flwr][INFO] - \n",
      "Federated simulation complete.\n",
      "Total reasoning summaries collected: 0\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python scripts/train_federated.py subset_size=50 num_clients=4 federated_rounds=2\n",
    "!python -m scripts.train_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3ee001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cuda\n",
      "output_dir: ${hydra:runtime.output_dir}\n",
      "num_clients: 5\n",
      "federated_rounds: 5\n",
      "batch_size: 4\n",
      "subset_size: 200\n",
      "data:\n",
      "  dataset_name: truthfulqa/truthful_qa\n",
      "  split: validation\n",
      "  task: multiple_choice\n",
      "model:\n",
      "  edge_model_name: microsoft/Phi-3-mini-4k-instruct\n",
      "  cloud_model_name: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  edge_quantization: 4bit\n",
      "  max_new_tokens: 256\n",
      "  temperature: 0.7\n",
      "federated:\n",
      "  learning_rate: 2.0e-05\n",
      "  local_epochs: 1\n",
      "  communication_tracking: true\n",
      "\n",
      "[2026-01-13 15:01:01,985][src.data.dataset][INFO] - Loading truthful_qa 'multiple_choice' - split: validation, subset: 200\n",
      "README.md: 9.59kB [00:00, 35.4MB/s]\n",
      "multiple_choice/validation-00000-of-0000(): 100%|| 271k/271k [00:01<00:00, 231\n",
      "Generating validation split: 100%|| 817/817 [00:00<00:00, 31503.36 examples/s]\n",
      "Formatting TruthfulQA multiple_choice examples: 100%|| 817/817 [00:00<00:00, 34\n",
      "[2026-01-13 15:01:08,990][src.data.dataset][INFO] - Subsampled to 200 examples\n",
      "[2026-01-13 15:01:08,990][__main__][INFO] - Evaluating on 200 examples\n",
      "[2026-01-13 15:01:08,998][src.models.edge_model][INFO] - Loading edge model: microsoft/Phi-3-mini-4k-instruct on cpu with 4bit quantization\n",
      "tokenizer_config.json: 3.44kB [00:00, 21.8MB/s]\n",
      "tokenizer.model: 100%|| 500k/500k [00:00<00:00, 1.04MB/s]\n",
      "tokenizer.json: 1.94MB [00:00, 121MB/s]\n",
      "added_tokens.json: 100%|| 306/306 [00:00<00:00, 2.22MB/s]\n",
      "special_tokens_map.json: 100%|| 599/599 [00:00<00:00, 5.13MB/s]\n",
      "config.json: 100%|| 967/967 [00:00<00:00, 10.1MB/s]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "model.safetensors.index.json: 16.5kB [00:00, 68.1MB/s]\n",
      "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/2.67G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 690k/4.97G [00:01<2:24:14, 574kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 93.4M/4.97G [00:01<01:04, 75.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|    | 5.42M/2.67G [00:01<12:49, 3.46MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|    | 9.46M/2.67G [00:01<07:33, 5.86MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|    | 133M/4.97G [00:01<00:56, 85.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   1%|    | 37.3M/2.67G [00:02<01:38, 26.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   2%|    | 51.2M/2.67G [00:03<02:18, 19.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|    | 156M/4.97G [00:03<02:03, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|    | 191M/4.97G [00:03<01:28, 53.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|    | 232M/4.97G [00:04<01:19, 59.3MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   3%|    | 73.3M/2.67G [00:04<02:27, 17.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|    | 261M/4.97G [00:04<01:16, 61.5MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   3%|   | 88.3M/2.67G [00:05<02:09, 19.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|    | 290M/4.97G [00:05<01:38, 47.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|    | 306M/4.97G [00:05<01:31, 51.1MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   4%|    | 119M/2.67G [00:07<02:47, 15.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   7%|    | 178M/2.67G [00:08<01:23, 30.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|    | 358M/4.97G [00:08<02:23, 32.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   7%|    | 191M/2.67G [00:08<01:13, 33.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|    | 394M/4.97G [00:09<02:30, 30.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|    | 489M/4.97G [00:10<01:26, 51.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|    | 590M/4.97G [00:11<01:12, 60.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|    | 639M/4.97G [00:12<01:20, 53.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   9%|    | 239M/2.67G [00:13<02:29, 16.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|    | 677M/4.97G [00:14<01:43, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  11%|    | 292M/2.67G [00:14<01:53, 21.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  13%|    | 336M/2.67G [00:16<01:47, 21.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|    | 803M/4.97G [00:16<01:26, 48.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|    | 937M/4.97G [00:17<00:53, 75.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|   | 1.00G/4.97G [00:20<01:21, 48.8MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  15%|    | 405M/2.67G [00:21<02:03, 18.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|   | 1.07G/4.97G [00:22<01:39, 39.1MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  18%|    | 468M/2.67G [00:23<01:45, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  20%|    | 532M/2.67G [00:24<01:17, 27.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  21%|    | 574M/2.67G [00:24<01:00, 34.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|   | 1.20G/4.97G [00:24<01:20, 46.8MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  24%|   | 641M/2.67G [00:25<00:44, 45.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  29%|   | 775M/2.67G [00:26<00:33, 56.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|   | 1.22G/4.97G [00:27<01:54, 32.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  32%|   | 842M/2.67G [00:27<00:28, 63.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  35%|   | 945M/2.67G [00:29<00:26, 65.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  37%|   | 989M/2.67G [00:30<00:28, 58.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  40%|  | 1.07G/2.67G [00:30<00:19, 82.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|   | 1.22G/4.97G [00:30<03:01, 20.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  43%|  | 1.14G/2.67G [00:32<00:25, 59.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|   | 1.28G/4.97G [00:32<02:46, 22.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  45%|  | 1.21G/2.67G [00:34<00:34, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|   | 1.34G/4.97G [00:35<02:31, 24.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|   | 1.35G/4.97G [00:35<02:39, 22.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|  | 1.43G/4.97G [00:36<01:50, 32.1MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  48%|  | 1.27G/2.67G [00:37<00:41, 33.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|  | 1.49G/4.97G [00:39<01:56, 30.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|  | 1.52G/4.97G [00:39<01:40, 34.4MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  53%|  | 1.41G/2.67G [00:40<00:30, 40.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  54%| | 1.45G/2.67G [00:42<00:33, 36.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  57%| | 1.53G/2.67G [00:43<00:28, 40.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|  | 1.60G/4.97G [00:44<02:24, 23.3MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  60%| | 1.60G/2.67G [00:45<00:28, 38.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  63%| | 1.68G/2.67G [00:46<00:18, 53.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|  | 1.67G/4.97G [00:48<02:42, 20.3MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  66%| | 1.75G/2.67G [00:48<00:22, 40.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  66%| | 1.77G/2.67G [00:49<00:22, 39.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  68%| | 1.81G/2.67G [00:49<00:18, 46.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  69%| | 1.84G/2.67G [00:50<00:16, 51.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|  | 1.74G/4.97G [00:51<02:22, 22.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|  | 1.78G/4.97G [00:52<02:10, 24.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|  | 1.84G/4.97G [00:53<01:42, 30.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|  | 1.85G/4.97G [00:53<01:41, 30.6MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  71%| | 1.89G/2.67G [00:53<00:27, 28.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|  | 1.90G/4.97G [00:54<01:37, 31.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|  | 1.94G/4.97G [00:55<01:13, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|  | 1.96G/4.97G [00:55<01:11, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|  | 2.03G/4.97G [00:57<01:18, 37.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|  | 2.06G/4.97G [00:58<01:21, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|  | 2.09G/4.97G [00:59<01:23, 34.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|  | 2.14G/4.97G [00:59<01:01, 46.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|  | 2.20G/4.97G [01:00<00:48, 57.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|  | 2.25G/4.97G [01:01<00:41, 66.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  71%| | 1.89G/2.67G [01:04<00:27, 28.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  73%| | 1.94G/2.67G [01:08<01:27, 8.39MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|  | 2.31G/4.97G [01:08<02:13, 19.9MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  75%| | 1.99G/2.67G [01:09<01:00, 11.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|  | 2.38G/4.97G [01:10<01:53, 22.9MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  77%| | 2.06G/2.67G [01:11<00:40, 15.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|  | 2.39G/4.97G [01:11<02:08, 20.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  79%|| 2.11G/2.67G [01:12<00:29, 19.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|  | 2.44G/4.97G [01:12<01:38, 25.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|  | 2.45G/4.97G [01:12<01:34, 26.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|  | 2.47G/4.97G [01:13<01:41, 24.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|  | 2.53G/4.97G [01:14<00:55, 44.1MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  81%|| 2.17G/2.67G [01:14<00:22, 22.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|  | 2.60G/4.97G [01:15<00:55, 43.0MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  82%|| 2.20G/2.67G [01:16<00:23, 20.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%| | 2.66G/4.97G [01:16<00:53, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%| | 2.67G/4.97G [01:17<00:51, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%| | 2.70G/4.97G [01:18<01:05, 34.7MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  82%|| 2.20G/2.67G [01:28<00:23, 20.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%| | 2.77G/4.97G [01:29<03:11, 11.5MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  84%|| 2.24G/2.67G [01:39<01:23, 5.18MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%| | 2.77G/4.97G [01:44<03:11, 11.5MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  84%|| 2.24G/2.67G [01:54<01:23, 5.18MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  85%|| 2.28G/2.67G [02:08<02:06, 3.07MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%| | 2.84G/4.97G [02:12<10:43, 3.32MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%| | 2.84G/4.97G [02:24<10:43, 3.32MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  85%|| 2.28G/2.67G [02:24<02:06, 3.07MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|  | 2.90G/4.97G [05:36<44:05, 783kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  87%|| 2.34G/2.67G [05:43<08:42, 639kB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%| | 2.96G/4.97G [05:44<31:20, 1.07MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  90%|| 2.40G/2.67G [05:44<04:21, 1.02MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%| | 3.02G/4.97G [05:45<21:17, 1.53MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%| | 3.05G/4.97G [05:45<16:53, 1.90MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%| | 3.13G/4.97G [05:46<09:48, 3.14MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%| | 3.18G/4.97G [05:46<07:11, 4.17MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%| | 3.26G/4.97G [05:47<04:04, 6.99MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  92%|| 2.47G/2.67G [05:47<02:12, 1.52MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%| | 3.31G/4.97G [05:47<02:59, 9.22MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%| | 3.36G/4.97G [05:47<02:14, 12.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%| | 3.38G/4.97G [05:47<01:55, 13.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%| | 3.42G/4.97G [05:48<01:30, 17.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  95%|| 2.54G/2.67G [05:48<00:59, 2.24MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%| | 3.48G/4.97G [05:49<01:08, 21.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%| | 3.50G/4.97G [05:50<01:00, 24.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%| | 3.56G/4.97G [05:51<00:44, 31.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%| | 3.62G/4.97G [05:51<00:32, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%| | 3.69G/4.97G [05:52<00:24, 53.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%| | 3.75G/4.97G [05:53<00:19, 62.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%| | 3.81G/4.97G [05:53<00:16, 68.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%| | 3.87G/4.97G [05:54<00:16, 67.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|| 3.94G/4.97G [05:55<00:15, 68.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|| 3.99G/4.97G [05:56<00:13, 73.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|| 4.02G/4.97G [05:56<00:13, 71.0MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  97%|| 2.60G/2.67G [05:57<00:23, 2.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors: 100%|| 2.67G/2.67G [05:58<00:00, 7.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  82%|| 4.09G/4.97G [05:59<00:18, 47.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|| 4.11G/4.97G [05:59<00:17, 50.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|| 4.17G/4.97G [06:00<00:15, 52.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|| 4.22G/4.97G [06:00<00:10, 69.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|| 4.26G/4.97G [06:02<00:17, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|| 4.30G/4.97G [06:02<00:12, 52.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|| 4.38G/4.97G [06:04<00:12, 47.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|| 4.44G/4.97G [06:07<00:14, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|| 4.51G/4.97G [06:07<00:09, 49.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|| 4.58G/4.97G [06:08<00:07, 52.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|| 4.64G/4.97G [06:09<00:05, 62.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|| 4.71G/4.97G [06:09<00:03, 84.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|| 4.78G/4.97G [06:09<00:01, 109MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|| 4.84G/4.97G [06:10<00:01, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|| 4.91G/4.97G [06:16<00:02, 28.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|| 4.97G/4.97G [06:17<00:00, 13.2MB/s]\u001b[A\n",
      "Fetching 2 files: 100%|| 2/2 [06:17<00:00, 188.82s/it]\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python -m scripts.evaluate_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.evaluate_hybrid subset_size=30 use_wandb=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.evaluate_local subset_size=100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
